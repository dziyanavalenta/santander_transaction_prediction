{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_paGd_yLbgH"
   },
   "source": [
    "# Santander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAuqPwTnLipr"
   },
   "source": [
    "# 1. Import libs\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUXGcC4KLmcL"
   },
   "outputs": [],
   "source": [
    "# pandas and numpy imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import warnings\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn.metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5LciKOr8Lo5O"
   },
   "source": [
    "# 2. Get data\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RlmPzZGLtGi"
   },
   "outputs": [],
   "source": [
    "bank_ds = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_test_ds = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop an unneeded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'column_to_drop' is the name of the column you want to drop\n",
    "column_to_drop = 'ID_code'\n",
    "\n",
    "# Drop the column from bank_ds\n",
    "bank_ds.drop(column_to_drop, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split 70/30 (stratified sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = bank_ds['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140000, 200), (60000, 200))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bank_ds.drop(labels=['target'], axis=1),  # drop the target\n",
    "    bank_ds['target'],  # just the target\n",
    "    test_size=0.3,\n",
    "    stratify=target,\n",
    "    random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models like Logistic Regression require scaling for better performance. Tree-based models like Random Forest, XGBoost, and LightGBM do not inherently require scaling but scaling can still be beneficial (can improve performance and convergence in some cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# we put the variables in the same scale\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML models\n",
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=2, n_jobs=4)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_rf = rf.predict_proba(X_train)[:,1]\n",
    "y_test_rf = rf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state=42,  max_iter=2000)\n",
    "\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "y_train_logit = logit.predict_proba(X_train)[:,1]\n",
    "y_test_logit = logit.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_xgb = xgb_model.predict_proba(X_train)[:, 1]\n",
    "y_test_xgb = xgb_model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 14069, number of negative: 125931\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 140000, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100493 -> initscore=-2.191760\n",
      "[LightGBM] [Info] Start training from score -2.191760\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lgb_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_lgb = lgb_model.predict_proba(X_train)[:, 1]\n",
    "y_test_lgb = lgb_model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "catboost_model = CatBoostClassifier(random_state=42, verbose=False)  # Disable logging\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the training and test sets\n",
    "y_train_catboost = catboost_model.predict_proba(X_train)[:, 1]\n",
    "y_test_catboost = catboost_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics which require predict method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "TN | FP\n",
    "\n",
    "FN | TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix, Recall, Precision, F1-score, G-mean, etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Recall Majority (TNR)  \\\n",
      "0             Baseline  0.899517               1.000000   \n",
      "1        Random Forest  0.899517               1.000000   \n",
      "2  Logistic Regression  0.913517               0.986085   \n",
      "3              XGBoost  0.913183               0.987660   \n",
      "4             LightGBM  0.907283               0.998295   \n",
      "\n",
      "   Recall Minority (TPR)  Balanced Accuracy       FPR       FNR  \\\n",
      "0               0.000000           0.500000  0.000000  1.000000   \n",
      "1               0.000000           0.500000  0.000000  1.000000   \n",
      "2               0.263891           0.624988  0.013915  0.736109   \n",
      "3               0.246475           0.617068  0.012340  0.753525   \n",
      "4               0.092553           0.545424  0.001705  0.907447   \n",
      "\n",
      "   Precision Majority  Precision Minority  F1-Score Majority  ...  \\\n",
      "0            0.899517            0.000000           0.947101  ...   \n",
      "1            0.899517            0.000000           0.947101  ...   \n",
      "2            0.923029            0.679334           0.953516  ...   \n",
      "3            0.921467            0.690520           0.953416  ...   \n",
      "4            0.907818            0.858462           0.950909  ...   \n",
      "\n",
      "   G-mean-binary  G-mean-weighted  G-mean-micro  G-mean-macro  \\\n",
      "0       0.000000         0.300643      0.899517      0.500000   \n",
      "1       0.000000         0.300643      0.899517      0.500000   \n",
      "2       0.510117         0.554402      0.913517      0.624988   \n",
      "3       0.493390         0.541376      0.913183      0.617068   \n",
      "4       0.303965         0.408099      0.907283      0.545424   \n",
      "\n",
      "   Corrected G-mean  Corrected G-mean-binary  Corrected G-mean-weighted  \\\n",
      "0          0.000000                 0.000000                   0.126497   \n",
      "1          0.000000                 0.000000                   0.126497   \n",
      "2          0.260219                 0.166255                   0.396044   \n",
      "3          0.243434                 0.153219                   0.379876   \n",
      "4          0.092395                 0.050552                   0.226811   \n",
      "\n",
      "   Corrected G-mean-micro  Corrected G-mean-macro  Dominance  \n",
      "0                0.809130                0.250000  -1.000000  \n",
      "1                0.809130                0.250000  -1.000000  \n",
      "2                0.834513                0.390610  -0.722194  \n",
      "3                0.833904                0.380773  -0.741185  \n",
      "4                0.823163                0.297487  -0.905743  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "                 Model              Confusion matrix\n",
      "0             Baseline       [[53971, 0], [6029, 0]]\n",
      "1        Random Forest       [[53971, 0], [6029, 0]]\n",
      "2  Logistic Regression  [[53220, 751], [4438, 1591]]\n",
      "3              XGBoost  [[53305, 666], [4543, 1486]]\n",
      "4             LightGBM    [[53879, 92], [5471, 558]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# require scaling\n",
    "from imblearn.metrics import (\n",
    "    geometric_mean_score,\n",
    "    make_index_balanced_accuracy,\n",
    ")\n",
    "\n",
    "def dominance(y_true, y_pred):\n",
    "    tpr = recall_score(y_test, y_pred, pos_label=1)\n",
    "    tnr = recall_score(y_test, y_pred, pos_label=0)\n",
    "    return tpr - tnr\n",
    "\n",
    "# Function to get classification metrics\n",
    "def get_classification_metrics(y_true, y_pred):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    accuracy          = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Adding possible averages to recall, precision, and f1-score calculations\n",
    "    # Default average is None - scores are returned for each class individually rather than being averaged\n",
    "    recall_weighted    = recall_score(y_test, y_pred, average='weighted')\n",
    "    precision_weighted = precision_score(y_test, y_pred, average='weighted') \n",
    "    f1_weighted        = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    recall_micro       = recall_score(y_test, y_pred, average='micro')\n",
    "    precision_micro    = precision_score(y_test, y_pred, average='micro')\n",
    "    f1_micro           = f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    recall_macro       = recall_score(y_test, y_pred, average='macro')\n",
    "    precision_macro    = precision_score(y_test, y_pred, average='macro')\n",
    "    f1_macro           = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Adding possible averages to G-mean\n",
    "    # If the average parameter is not specified, it defaults to average='binary'.\n",
    "    g_mean_binary      = geometric_mean_score(y_test, y_pred, average='binary')\n",
    "    g_mean_weighted    = geometric_mean_score(y_test, y_pred, average='weighted')\n",
    "    g_mean_micro       = geometric_mean_score(y_test, y_pred, average='micro')\n",
    "    g_mean_macro       = geometric_mean_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # A lower alpha gives more weight to sensitivity (TPR), while a higher alpha gives more weight to specificity (TNR). \n",
    "    # In other words,in case of a lower alpha more emphasis is placed on correctly identifying positive instances\n",
    "    gmean = make_index_balanced_accuracy(alpha=0.5, squared=True)(geometric_mean_score) \n",
    "    corrected_g_mean  = gmean(y_test, y_pred)\n",
    "    # specifying an average might not be necessary or meaningful. Leaving for the time being\n",
    "    corrected_g_mean_binary  = gmean(y_test, y_pred,average='binary')\n",
    "    corrected_g_mean_weighted  = gmean(y_test, y_pred,average='weighted')\n",
    "    corrected_g_mean_micro  = gmean(y_test, y_pred,average='micro')\n",
    "    corrected_g_mean_macro  = gmean(y_test, y_pred,average='macro')\n",
    "    \n",
    "    dominance_score   = dominance(y_test, y_pred) \n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()\n",
    "    FPR = fp / (tn + fp)\n",
    "    FNR = fn / (tp + fn)\n",
    "    # TPR = tp / (tp + fn) # recallminority\n",
    "    # TNR = tn / (tn + fp) # recall majority\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Recall Majority (TNR)': report['0']['recall'],\n",
    "        'Recall Minority (TPR)': report['1']['recall'],\n",
    "        'Balanced Accuracy': balanced_accuracy,\n",
    "        'FPR': FPR,\n",
    "        'FNR': FNR,\n",
    "        \n",
    "        'Precision Majority': report['0']['precision'],\n",
    "        'Precision Minority': report['1']['precision'],\n",
    "        'F1-Score Majority': report['0']['f1-score'],\n",
    "        'F1-Score Minority': report['1']['f1-score'], \n",
    "        \n",
    "        # Weighted average (Use 'weighted' if you want to take class imbalance into account)\n",
    "        'Weighted Precision': precision_weighted,\n",
    "        'Weighted Recall': recall_weighted,\n",
    "        'Weighted F1-Score': f1_weighted,\n",
    "\n",
    "        # Micro average (Use 'micro' if you want a metric that gives equal weight to every individual prediction)\n",
    "        'Micro Precision': precision_micro,\n",
    "        'Micro Recall': recall_micro,\n",
    "        'Micro F1-Score': f1_micro,\n",
    "        \n",
    "        # Macro average (Use 'macro' if you want to treat each class equally)\n",
    "        'Macro Precision': precision_macro,\n",
    "        'Macro Recall': recall_macro,\n",
    "        'Macro F1-Score': f1_macro,\n",
    "\n",
    "        # Geometric average (a suitable metric when you want to balance the trade-off between detecting the minority class \n",
    "        # and avoiding false positives from the majority class)\n",
    "        'G-mean-binary': g_mean_binary, # (compute two geometric means for each class, then the arithmetic mean of these G-means)\n",
    "        'G-mean-weighted': g_mean_weighted, # calculates the geometric mean for each class and then computes the weighted arithmetic mean of these G-means\n",
    "        'G-mean-micro': g_mean_micro, # treats all classes equally, regardless of their size\n",
    "        'G-mean-macro': g_mean_macro, # use when you want to weight each instance equally, regardless of their class\n",
    "\n",
    "        # Corrected G-means\n",
    "        'Corrected G-mean': corrected_g_mean,\n",
    "        'Corrected G-mean-binary': corrected_g_mean_binary,\n",
    "        'Corrected G-mean-weighted': corrected_g_mean_weighted,\n",
    "        'Corrected G-mean-micro': corrected_g_mean_micro,\n",
    "        'Corrected G-mean-macro': corrected_g_mean_macro,\n",
    "        \n",
    "        'Dominance': dominance_score\n",
    "    }\n",
    "\n",
    "models = {\n",
    "    'Baseline': y_test_base,\n",
    "    'Random Forest': rf.predict(X_test),\n",
    "    'Logistic Regression': logit.predict(X_test),\n",
    "    'XGBoost': xgb_model.predict(X_test),\n",
    "    'LightGBM': lgb_model.predict(X_test),\n",
    "    'CatBoost': catboost_model.predict(X_test)\n",
    "}\n",
    "\n",
    "# Collecting results\n",
    "results = []\n",
    "confusion_matricies = []\n",
    "for model_name, y_pred in models.items():\n",
    "    metrics           = get_classification_metrics(y_test, y_pred)\n",
    "    conf_matrix       = confusion_matrix(y_test, y_pred)\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        **metrics,\n",
    "    })\n",
    "    confusion_matricies.append({\n",
    "        'Model': model_name,\n",
    "        'Confusion matrix': conf_matrix\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results_1 = pd.DataFrame(results)\n",
    "df_confusion_matricies = pd.DataFrame(confusion_matricies)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(df_results_1)\n",
    "print(df_confusion_matricies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics which require predict_proba method\n",
    "The ROC AUC score is calculated based on the predicted probabilities for each class, which are obtained using the predict_proba method. This method provides the probability estimates for each class, which are necessary to compute the ROC curve and subsequently the AUC (Area Under the Curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model   ROC-AUC    PR-AUC\n",
      "0             Baseline  0.500000  0.100483\n",
      "1        Random Forest  0.748215  0.277840\n",
      "2  Logistic Regression  0.858844  0.498939\n",
      "3              XGBoost  0.854883  0.488493\n",
      "4             LightGBM  0.863880  0.511203\n",
      "5             CatBoost  0.893906  0.592294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'Baseline': y_test_base,\n",
    "    'Random Forest': rf.predict_proba(X_test)[:,1],\n",
    "    'Logistic Regression': logit.predict_proba(X_test)[:,1],\n",
    "    'XGBoost': xgb_model.predict_proba(X_test)[:,1],\n",
    "    'LightGBM': lgb_model.predict_proba(X_test)[:,1],\n",
    "    'CatBoost': catboost_model.predict_proba(X_test)[:,1]\n",
    "}\n",
    "\n",
    "# Collecting results\n",
    "results = []\n",
    "for model_name, y_pred in models.items():\n",
    "    roc_auc   = roc_auc_score(y_test, y_pred) \n",
    "    pr_auc = average_precision_score(y_test, y_pred)\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results_2 = pd.DataFrame(results)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(df_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-sensitive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 14069, number of negative: 125931\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 140000, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "                 Model   ROC-AUC    PR-AUC\n",
      "0        Random Forest  0.777911  0.300048\n",
      "1  Logistic Regression  0.858523  0.498147\n",
      "2              XGBoost  0.851339  0.480928\n",
      "3             LightGBM  0.868421  0.520339\n",
      "4             CatBoost  0.891601  0.587179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "# Assuming ratio is the ratio of the number of negative samples to positive samples\n",
    "ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "# Train the models with cost-sensitive approach\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "logit = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "xgb_model = XGBClassifier(scale_pos_weight=ratio)\n",
    "lgb_model = LGBMClassifier(class_weight='balanced')\n",
    "catboost_model = CatBoostClassifier(auto_class_weights='Balanced', verbose=False)\n",
    "\n",
    "# Fit the models\n",
    "rf.fit(X_train, y_train)\n",
    "logit.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "models = {\n",
    "    'Random Forest': rf.predict_proba(X_test)[:, 1],\n",
    "    'Logistic Regression': logit.predict_proba(X_test)[:, 1],\n",
    "    'XGBoost': xgb_model.predict_proba(X_test)[:, 1],\n",
    "    'LightGBM': lgb_model.predict_proba(X_test)[:, 1],\n",
    "    'CatBoost': catboost_model.predict_proba(X_test)[:, 1]\n",
    "}\n",
    "\n",
    "# Collecting results\n",
    "results = []\n",
    "for model_name, y_pred in models.items():\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    pr_auc = average_precision_score(y_test, y_pred)\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results_2 = pd.DataFrame(results)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(df_results_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df_results_1 and df_results_2 horisontally\n",
    "df_combined_results = pd.concat([df_results_2, df_results_1], axis=1)\n",
    "\n",
    "# Export results to CSV files\n",
    "df_combined_results.to_csv('3_evaluation_results_scaling.csv', index=False)\n",
    "df_confusion_matricies.to_csv('3_confusion_matricies_scaling.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN979dFKn9B6Ro9v0hJ4uqU",
   "name": "K-Means Clustering",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
